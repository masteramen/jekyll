---
layout: post
title:  "Spark基础-parallelize函数和makeRDD函数的区别"
title2:  "Spark基础-parallelize函数和makeRDD函数的区别"
date:   2017-01-01 23:49:37  +0800
source:  "https://www.jfox.info/spark%e5%9f%ba%e7%a1%80-parallelize%e5%87%bd%e6%95%b0%e5%92%8cmakerdd%e5%87%bd%e6%95%b0%e7%9a%84%e5%8c%ba%e5%88%ab.html"
fileName:  "20170100877"
lang:  "zh_CN"
published: true
permalink: "2017/spark%e5%9f%ba%e7%a1%80-parallelize%e5%87%bd%e6%95%b0%e5%92%8cmakerdd%e5%87%bd%e6%95%b0%e7%9a%84%e5%8c%ba%e5%88%ab.html"
---
{% raw %}
Spark提供了两种创建RDD的方式：读取外部数据集，以及在驱动器程序中对一个集合进行并行化。

在驱动器程序中对一个集合进行并行化的方式有两种：parallelize()和makeRDD()。

## 1、parallelize()

    def parallelize[T: ClassTag](
          seq: Seq[T],
          numSlices: Int = defaultParallelism): RDD[T] = withScope {
        assertNotStopped()
        new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())
      }

## 2、makeRDD()，有两种重构方法，如下：

**2.1、方法一：**

    /** Distribute a local Scala collection to form an RDD.
       *
       * This method is identical to `parallelize`.
       */
      def makeRDD[T: ClassTag](
          seq: Seq[T],
          numSlices: Int = defaultParallelism): RDD[T] = withScope {
        parallelize(seq, numSlices)
      }
    

 可以发现，该重构方法的实现就是调用parallelize()方法。

**2.2、方法二：**

    /**
       * Distribute a local Scala collection to form an RDD, with one or more
       * location preferences (hostnames of Spark nodes) for each object.
       * Create a new partition for each collection item.
       */
      def makeRDD[T: ClassTag](seq: Seq[(T, Seq[String])]): RDD[T] = withScope {
        assertNotStopped()
        val indexToPrefs = seq.zipWithIndex.map(t => (t._2, t._1._2)).toMap
        new ParallelCollectionRDD[T](this, seq.map(_._1), math.max(seq.size, 1), indexToPrefs)
      }

注释的意思为：分配一个本地Scala集合形成一个RDD，为每个**集合对象**创建一个最佳分区。

给出如下例子，可以更清晰的看到它们之间的区别：

首先定义集合对象：

    val seq = List(("American Person", List("Tom", "Jim")), ("China Person", List("LiLei", "HanMeiMei")), ("Color Type", List("Red", "Blue")))

使用parallelize()创建RDD：

    val rdd1 = sc.parallelize(seq)

查询rdd1的分区数：

    rdd1.partitions.size  // 2

使用makeRDD()创建RDD

    val rdd2 = sc.makeRDD(seq)

查看rdd2的分区数

    rdd2.partitions.size  // 3

想想为什么会是这样的结果？

当调用parallelize()方法的时候，不指定分区数的时候，使用系统给出的分区数，而调用makeRDD()方法的时候，会为每个集合对象创建最佳分区，而这对后续的调用优化很有帮助。
{% endraw %}